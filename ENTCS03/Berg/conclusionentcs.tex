\section{Conclusions and future work}

Among the conclusions we draw from our experiences is the fact that
random prefix-closed automata are harder to learn in comparison to
completely randomly generated automata. For our random examples, the
number of membership queries can roughly be described as linear
in the number of transitions. Membership queries for our prefix-closed
examples, in comparison, are approximately quadratic in transitions.

Moving deeper into the domain of prefix-closed automata we conclude
that it is possible to reduce the number of membership queries by
using an optimization specially shaped for these automata. The
optimization reduces the number of membership queries
considerably. For the randomly generated prefix-closed automata we
measured a reduction of about 20\%.

Turning our attention to the real-world examples we see that the
optimization works much better, saving 60\% membership queries
relative to unoptimized learning. We also compared the result of
learning real-world examples with randomly generated prefix-closed
examples of the same size, in order to investigate if they behaved in
the same manner. The result reveals a better performance for the
real-world examples in terms of membership queries, especially with
the optimization. This seems to imply that our real-world examples
have a more suited structure for learning. Hopefully this observation
can be used to optimize the learning process further.

Memory consumption is a problem we experienced when learning large
models. In order to learn these models, we need more memory efficient
data structures. Further optimizations for prefix-closed DFA are
possible. For instance, one can save space and time by using the fact
that there is exactly one non-final state.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
