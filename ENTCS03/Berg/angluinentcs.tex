\section{Learning finite state machines}

\label{sec:angluin}

\subsection{Angluin's learning algorithm}

\label{sec:angluin:direct}

We here try to give a succinct description of the main ideas behind
Angluin's learning algorithm.  We assume that a system in which we are
interested can be modeled by a DFA $\auto$. The problem can now be
looked upon as identifying the regular language which is accepted by
$\auto$, denoted by $\langP{\auto}$.

In a learning algorithm a so called
$Learner$, who initially knows nothing about $\auto$,
is trying to learn $\langP{\auto}$ by asking 
queries to a $Teacher$ and an $Oracle$.
There are two kinds of queries.
\begin{itemize}
\item A {\em membership query} consists in asking the $Teacher$ 
whether a string $w \in \alphabet^*$ is in $\langP{\auto}$.
\item An {\em equivalence query} consists in asking the $Oracle$ 
whether a hypothesized DFA $\hypo$ is correct, i.e., whether
$\langP{\hypo} = \langP{\auto}$.
The $Oracle$ will answer $yes$ if $\hypo$ is correct,
or else supply a counterexample
$u$, either in $\langP{\auto} \setminus \langP{\hypo}$
or in $\langP{\hypo} \setminus \langP{\auto}$.
\end{itemize}
The typical behavior of a $Learner$ is to start by asking a sequence
of membership queries, and gradually build a hypothesized DFA $\hypo$ using
the obtained answers. When the $Learner$ feels that she has
built a ``stable'' hypothesis $\hypo$, she makes
an equivalence query to find out whether $\hypo$ is correct.
If the result is successful, the $Learner$ has succeeded, otherwise
she uses the returned counterexample to revise $\hypo$ and perform
subsequent membership queries until
arriving at a new hypothesized DFA, etc.

The information gained by the $Learner$ can during the learning
process be represented as a partial mapping $\observations$ from
$\alphabet^*$ to $\set{\mbox{\it accepted},\mbox{\it rejected}}$.  The
domain $\domof{\observations}$ of $\observations$ is the set of
strings for which membership queries have been performed, or which the
$Oracle$ has given as counterexamples in equivalence queries.

Roughly speaking, a learning algorithm should prescribe how to transform
a partial mapping $\observations$ into an automaton.
This can be done by fixing a subset $S$ of 
$\domof{\observations}$, defining 
an equivalence relation $\simeq$ on $S$, and
building the automaton as the set of equivalence classes of strings
in $S$. Intuitively, two strings $u$ and $u'$
should be equivalent if there is reason to believe that
$u \equiv_{\langP{\auto}} u'$. Since the $Learner$ can only obtain partial
information about $\auto$ from $\observations$,
one idea is to approximate $\equiv_{\langP{\auto}}$ by an equivalence
$\simeq$, which uses only information in $\observations$. 
To be able to build an automaton on the basis of $S$ and
$\simeq$,
the following two criteria should preferably be satisfied.
\begin{itemize}
\item {\em completeness:} If $u \in S$, and $a \in \alphabet$ then 
$ua \simeq u'$ for some $u' \in S$.
\item {\em consistency:} If $u \simeq u'$ for $u,u' \in S$ and
$a \in \alphabet$, then $ua \simeq u'a$ (i.e., $\simeq$ is a right
congruence).
\end{itemize}
Completeness ensures that we can define transitions from
each equivalence class for each letter in $\alphabet$;
consistency ensures that such transitions have a unique target
equivalence class.
We note that in order to check completeness and consistency, it is necessary
to define $\simeq$ on all strings $u$ and $ua$ such that
$u \in S$ and $a \in \alphabet$.
Whenever the current
values of $S$ and $\simeq$ satisfy the completeness and consistency
criteria, the $Learner$ can form the corresponding 
hypothesis $\hypo$ and make an equivalence query about $\hypo$.

Let us now describe Angluin's algorithm more specifically.  Angluin's
algorithm maintains a prefix-closed set $S$ and a suffix-closed set $E$ of
strings, both of which are monotonically increased during the
algorithm. Initially $S$ and $E$ contain the empty string $\varepsilon$.
We define $\simeq$ as follows: $u \simeq v$ if
for all $w \in E$ we have $uw \in \langP{\cA}$ iff $vw \in
\langP{\cA}$.

From a complete and consistent $\observations$, a hypothesis $\hypo$
is formed as the automaton, whose states are equivalence classes of
strings in $S$.
If $\observations$ is not complete, then S is increased with
strings that represent missing equivalence classes.
If $\observations$ is not consistent, $E$ is increased with a suffix
which replaces the inconsistent equivalence class with two new classes.

The description of Angluin's algorithm in~\cite{Angluin:regset}
represents $\observations$ by an {\em observation table}
$\obstable$. The observation table is a table with rows corresponding
to strings in $S$ and columns corresponding to strings in $E$.  The
algorithm gradually fills the entry $(u,v)$ for row $u$ and column $v$
by {\em accepted} or {\em rejected}, after receiving a reply for a
membership query for $uv$.

Of course, some membership queries can be saved by entering also the
counterexamples returned in negative equivalence queries as accepted
or rejected.  We note that the observation table is redundant in
that the result of a membership query for $u$ occurs in all entries
$(v,w)$ such that $u = vw$. Thus, we do not need to make one
membership query for each such entry, but we can simultaneously fill
all such entries.

Angluin's algorithm is designed to construct minimal DFA for the
guessed language.

\subsection{Prefix-closed models}
\label{sect:prefix}

In many applications, we want to learn an automaton $\auto$, which is
a model of a reactive system. Often, reactive systems can be modeled
as transition systems. These can be understood as (non-deterministic)
finite-state automata (with partial transition relations) in which
every state is an accepting state.  Thus, the language defined by such
an automaton will be prefix-closed. In this section, we discuss how to
exploit this fact for optimizing the learning process.

A language $\Lang$ is \Def{prefix-closed} if for every $w$ in $\Lang$,
all prefixes of $w$ are in $\Lang$.  A DFA is \Def{prefix-closed} if
its language is prefix-closed.  It follows that a minimal
prefix-closed DFA has only one non-final state, the so-called
\Def{sink}, with transitions only to itself. Note that Angluin's
algorithm learns minimal DFA.

Studying strings that are possibly accepted by prefix-closed DFA, we
make the following simple but important observations:
\begin{enumerate}
\item \label{opt:acc} Prefixes of accepted strings are accepted.
\item \label{opt:rej} Extensions of rejected strings are rejected.
\end{enumerate}

We can use these characteristics of prefix-closed DFA to reduce the
needed number of membership queries as follows. Before querying a
string, we first test it for (\ref{opt:rej}), that is whether it is an
extension of a string already observed to be rejected. If so, we can
add the result immediately to the observation table. Otherwise, we ask
the teacher. Thus, we never need to query extensions of observed
rejected strings.

Angluin's $Learner$ starts with queries for short strings, and
thereafter queries successively longer and longer strings. In general,
the test for (\ref{opt:acc}) will not be able to consult previous
observations, so it is rarely applicable. There is, though, an
exception when it could be useful, namely when performing queries for
prefixes of received counterexamples. If the counterexample $c$ is
accepted, we know that all its prefixes are accepted, too. In the best
case, applying (\ref{opt:acc}) would save $m e$ membership queries,
where $m$ is the maximum length of any received counterexample and $e$
is the number of equivalence queries made. Knowing the best case
bound and due to lack of evaluation time, we did not implement (\ref{opt:acc}).

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
